{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22f22231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\bhavy\\anaconda32\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bhavy\\anaconda32\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bhavy\\anaconda32\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\bhavy\\anaconda32\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bhavy\\anaconda32\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bhavy\\anaconda32\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bhavy\\anaconda32\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bhavy\\anaconda32\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\bhavy\\anaconda32\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b5b5387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bhavy\\anaconda32\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import mediapipe as mp\n",
    "import math\n",
    "import numpy as np \n",
    "import os \n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf227202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(p1, p2):\n",
    "    ''' Calculate distance between two points\n",
    "    :param p1: First Point \n",
    "    :param p2: Second Point\n",
    "    :return: Euclidean distance between the points. (Using only the x and y coordinates).\n",
    "    '''\n",
    "    return (((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25a1a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(landmarks, eye):\n",
    "    ''' Calculate the ratio of the eye length to eye width. \n",
    "    :param landmarks: Face Landmarks returned from FaceMesh MediaPipe model\n",
    "    :param eye: List containing positions which correspond to the eye\n",
    "    :return: Eye aspect ratio value\n",
    "    '''\n",
    "    N1 = distance(landmarks[eye[1][0]], landmarks[eye[1][1]])\n",
    "    N2 = distance(landmarks[eye[2][0]], landmarks[eye[2][1]])\n",
    "    N3 = distance(landmarks[eye[3][0]], landmarks[eye[3][1]])\n",
    "    D = distance(landmarks[eye[0][0]], landmarks[eye[0][1]])\n",
    "    return (N1 + N2 + N3) / (3 * D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ed2618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_feature(landmarks, eye):\n",
    "    ''' Calculate the eye feature as the average of the eye aspect ratio for the two eyes\n",
    "    :param landmarks: Face landmarks returned from FaceMesh MediaPipe model\n",
    "    :param eye: List containing positions which correspond to the eye\n",
    "    :return: Eye feature value\n",
    "    '''\n",
    "    return (eye_aspect_ratio(landmarks, eye) + \\\n",
    "            eye_aspect_ratio(landmarks, eye))/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a27f1e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mouth_feature(landmarks, mouth):\n",
    "    ''' Calculate the mouth feature\n",
    "    :param landmarks: Face landmarks returned from FaceMesh MediaPipe model\n",
    "    :param mouth: List containing positions which correspond to the mouth\n",
    "    :return: Mouth feature value\n",
    "    '''\n",
    "    # Your implementation of mouth feature calculation here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5bd162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pupil_feature(landmarks):\n",
    "    ''' Calculate the pupil feature as the average of the pupil circularity for the two eyes\n",
    "    :param landmarks: Face Landmarks returned from FaceMesh MediaPipe model\n",
    "    :return: Pupil feature value\n",
    "    '''\n",
    "    return (pupil_circularity(landmarks, left_eye) + \\\n",
    "        pupil_circularity(landmarks, right_eye))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8eb2af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pupil_circularity(landmarks, eye):\n",
    "    ''' Calculate pupil circularity feature.\n",
    "    :param landmarks: Face Landmarks returned from FaceMesh MediaPipe model\n",
    "    :param eye: List containing positions which correspond to the eye\n",
    "    :return: Pupil circularity for the eye coordinates\n",
    "    '''\n",
    "    perimeter = distance(landmarks[eye[0][0]], landmarks[eye[1][0]]) + \\\n",
    "            distance(landmarks[eye[1][0]], landmarks[eye[2][0]]) + \\\n",
    "            distance(landmarks[eye[2][0]], landmarks[eye[3][0]]) + \\\n",
    "            distance(landmarks[eye[3][0]], landmarks[eye[0][1]]) + \\\n",
    "            distance(landmarks[eye[0][1]], landmarks[eye[3][1]]) + \\\n",
    "            distance(landmarks[eye[3][1]], landmarks[eye[2][1]]) + \\\n",
    "            distance(landmarks[eye[2][1]], landmarks[eye[1][1]]) + \\\n",
    "            distance(landmarks[eye[1][1]], landmarks[eye[0][0]])\n",
    "    area = math.pi * ((distance(landmarks[eye[1][0]], landmarks[eye[3][1]]) * 0.5) ** 2)\n",
    "    return (4*math.pi*area)/(perimeter**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c9557fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_face_mp(image):\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as face_mesh:\n",
    "        \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image_rgb)\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=None,\n",
    "                    landmark_drawing_spec=drawing_spec,\n",
    "                    connection_drawing_spec=drawing_spec)\n",
    "\n",
    "                landmarks_positions = []\n",
    "                for landmark in face_landmarks.landmark:\n",
    "                    landmarks_positions.append((landmark.x, landmark.y, landmark.z))\n",
    "                \n",
    "            ear, mar, puc, moe = None, None, None, None  # Initialize variables to None\n",
    "\n",
    "           # Calculate features\n",
    "            ear = eye_feature(landmarks_positions, left_eye)\n",
    "            mar = mouth_feature(landmarks_positions, mouth)\n",
    "            puc = pupil_feature(landmarks_positions)\n",
    "\n",
    "            if ear is not None and mar is not None and ear != 0:  # Ensure ear is not None or 0\n",
    "                moe = mar / ear\n",
    "            else:\n",
    "                moe = None  # Set moe to None if ear is None or 0\n",
    "\n",
    "            return ear, mar, puc, moe, image\n",
    "            \n",
    "    return -1000, -1000, -1000, -1000, image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d2967f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calibrate(calib_frame_count=100):\n",
    "    ears = []\n",
    "    mars = []\n",
    "    pucs = []\n",
    "    moes = []\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while len(ears) < calib_frame_count:\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        ear, mar, puc, moe, image = run_face_mp(image)\n",
    "        if ear != -1000:\n",
    "            ears.append(ear)\n",
    "            mars.append(mar)\n",
    "            pucs.append(puc)\n",
    "            moes.append(moe)\n",
    "\n",
    "        cv2.imshow('Calibration', image)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Filter out None values and compute mean and standard deviation\n",
    "    ears_filtered = [value for value in ears if value is not None]\n",
    "    mars_filtered = [value for value in mars if value is not None]\n",
    "    pucs_filtered = [value for value in pucs if value is not None]\n",
    "    moes_filtered = [value for value in moes if value is not None]\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    ears_norm = [np.mean(ears_filtered), np.std(ears_filtered)]\n",
    "    mars_norm = [np.mean(mars_filtered), np.std(mars_filtered)]\n",
    "    pucs_norm = [np.mean(pucs_filtered), np.std(pucs_filtered)]\n",
    "    moes_norm = [np.mean(moes_filtered), np.std(moes_filtered)]\n",
    "\n",
    "    return ears_norm, mars_norm, pucs_norm, moes_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb9f7c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification(input_data):\n",
    "    ''' Perform classification over the facial  features.\n",
    "    :param input_data: List of facial features for 20 frames\n",
    "    :return: Alert / Drowsy state prediction\n",
    "    '''\n",
    "    model_input = []\n",
    "    model_input.append(input_data[:5])\n",
    "    model_input.append(input_data[3:8])\n",
    "    model_input.append(input_data[6:11])\n",
    "    model_input.append(input_data[9:14])\n",
    "    model_input.append(input_data[12:17])\n",
    "    model_input.append(input_data[15:])\n",
    "    model_input = torch.FloatTensor(np.array(model_input))\n",
    "    preds = torch.sigmoid(model(model_input)).gt(0.5).int().data.numpy()\n",
    "    return int(preds.sum() >= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da67597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting calibration. Please be in neutral state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhavy\\anaconda32\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\bhavy\\anaconda32\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\bhavy\\anaconda32\\Lib\\site-packages\\numpy\\core\\_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\bhavy\\anaconda32\\Lib\\site-packages\\numpy\\core\\_methods.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "C:\\Users\\bhavy\\anaconda32\\Lib\\site-packages\\numpy\\core\\_methods.py:261: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main application\n",
      "got label  0\n",
      "got label  0\n",
      "got label  0\n",
      "got label  0\n",
      "got label  0\n",
      "got label  0\n",
      "got label  0\n",
      "got label  1\n",
      "got label  0\n",
      "got label  0\n",
      "got label  1\n",
      "got label  1\n",
      "got label  0\n",
      "got label  0\n",
      "got label  0\n",
      "got label  0\n"
     ]
    }
   ],
   "source": [
    "def infer(ears_norm, mars_norm, pucs_norm, moes_norm):\n",
    "    ''' Perform inference.\n",
    "    :param ears_norm: Normalization values for eye feature\n",
    "    :param mars_norm: Normalization values for mouth feature\n",
    "    :param pucs_norm: Normalization values for pupil feature\n",
    "    :param moes_norm: Normalization values for mouth over eye feature. \n",
    "    '''\n",
    "    ear_main = 0\n",
    "    mar_main = 0\n",
    "    puc_main = 0\n",
    "    moe_main = 0\n",
    "    decay = 0.9 # use decay to smoothen the noise in feature values\n",
    "\n",
    "    label = None\n",
    "\n",
    "    input_data = []\n",
    "    frame_before_run = 0\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        ear, mar, puc, moe, image = run_face_mp(image)\n",
    "        if ear != -1000:\n",
    "            ear = (ear - ears_norm[0])/ears_norm[1] \n",
    "            if mar is not None:  # Add this check\n",
    "                mar = (mar - mars_norm[0])/mars_norm[1]\n",
    "            else:\n",
    "                default_mar_value = 0  # You can set any default value you prefer\n",
    "                mar = default_mar_value \n",
    "            puc = (puc - pucs_norm[0])/pucs_norm[1]\n",
    "            if moe is not None:  # Add this check\n",
    "                moe = (moe - moes_norm[0])/moes_norm[1]\n",
    "            else:\n",
    "                default_moe_value = 0\n",
    "                moe = default_moe_value\n",
    "            if ear_main == -1000:\n",
    "                ear_main = ear\n",
    "                mar_main = mar\n",
    "                puc_main = puc\n",
    "                moe_main = moe\n",
    "            else:\n",
    "                ear_main = ear_main*decay + (1-decay)*ear\n",
    "                mar_main = mar_main*decay + (1-decay)*mar\n",
    "                puc_main = puc_main*decay + (1-decay)*puc\n",
    "                moe_main = moe_main*decay + (1-decay)*moe\n",
    "        else:\n",
    "            ear_main = -1000\n",
    "            mar_main = -1000\n",
    "            puc_main = -1000\n",
    "            moe_main = -1000\n",
    "        \n",
    "        if len(input_data) == 20:\n",
    "            input_data.pop(0)\n",
    "        input_data.append([ear_main, mar_main, puc_main, moe_main])\n",
    "\n",
    "        frame_before_run += 1\n",
    "        if frame_before_run >= 15 and len(input_data) == 20:\n",
    "            frame_before_run = 0\n",
    "            label = get_classification(input_data)\n",
    "            print ('got label ', label)\n",
    "        \n",
    "        cv2.putText(image, \"EAR: %.2f\" %(ear_main), (int(0.02*image.shape[1]), int(0.07*image.shape[0])),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "        cv2.putText(image, \"MAR: %.2f\" %(mar_main), (int(0.27*image.shape[1]), int(0.07*image.shape[0])),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "        cv2.putText(image, \"PUC: %.2f\" %(puc_main), (int(0.52*image.shape[1]), int(0.07*image.shape[0])),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "        cv2.putText(image, \"MOE: %.2f\" %(moe_main), (int(0.77*image.shape[1]), int(0.07*image.shape[0])),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2)\n",
    "        if label is not None:\n",
    "            if label == 0:\n",
    "                color = (0, 255, 0)\n",
    "            else:\n",
    "                color = (0, 0, 255)\n",
    "            cv2.putText(image, \"%s\" %(states[label]), (int(0.02*image.shape[1]), int(0.2*image.shape[0])),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.5, color, 2)\n",
    "\n",
    "        cv2.imshow('MediaPipe FaceMesh', image)\n",
    "        if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "\n",
    "    \n",
    "right_eye = [[33, 133], [160, 144], [159, 145], [158, 153]] # right eye landmark positions\n",
    "left_eye = [[263, 362], [387, 373], [386, 374], [385, 380]] # left eye landmark positions\n",
    "mouth = [[61, 291], [39, 181], [0, 17], [269, 405]] # mouth landmark coordinates\n",
    "states = ['alert', 'drowsy']\n",
    "\n",
    "# Declaring FaceMesh model\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    min_detection_confidence=0.3, min_tracking_confidence=0.8)\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "model_lstm_path = \"C:\\\\Users\\\\bhavy\\\\cnn\\\\clf_lstm_jit6 (3).pth\"\n",
    "model = torch.jit.load(model_lstm_path)\n",
    "model.eval()\n",
    "\n",
    "print ('Starting calibration. Please be in neutral state')\n",
    "time.sleep(5)\n",
    "ears_norm, mars_norm, pucs_norm, moes_norm = calibrate()\n",
    "\n",
    "print ('Starting main application')\n",
    "time.sleep(1)\n",
    "infer(ears_norm, mars_norm, pucs_norm, moes_norm)\n",
    "\n",
    "face_mesh.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d60c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a28ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
